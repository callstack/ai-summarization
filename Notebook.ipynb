{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20acd3a6-6a69-4886-8101-4577c9e004d4",
   "metadata": {},
   "source": [
    "### Input\n",
    "Set either INPUT_FILE or INPUT_TEXT variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1035d2-1cda-477e-886e-a491d6c81cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide either INPUT_FILE path or INPUT_TEXT to summarize.\n",
    "INPUT_FILE=\"./inputs/Wiki.txt\" # Insert file path here\n",
    "INPUT_TEXT=\"\"\"Insert text to summarize here.\"\"\"\n",
    "\n",
    "# Style of summarization:\n",
    "\n",
    "# Numbered List style\n",
    "STYLE=\"Return your response as numbered list which covers the main points of the text.\"\n",
    "PROMPT_TRIGGER=\"NUMBERED LIST SUMMARY\"\n",
    "\n",
    "# One sentence style\n",
    "# STYLE=\"Return your response as one sentence which covers the main points of the text.\",\n",
    "# PROMPT_TRIGGER=\"ONE SENTENCE SUMMARY\",\n",
    "\n",
    "# Concise style\n",
    "# STYLE=\"Return your response as concise summary which covers the main points of the text.\",\n",
    "# PROMPT_TRIGGER=\"CONCISE SUMMARY\",\n",
    "\n",
    "# Detailed style\n",
    "# STYLE=\"Return your response as detailed summary which covers the main points of the text and key facts and figures.\",\n",
    "# PROMPT_TRIGGER=\"DETAILED SUMMARY\",\n",
    "\n",
    "# Output language, try e.g. Polish, Spanish, etc \n",
    "OUTPUT_LANGUAGE = \"English\"\n",
    "\n",
    "# Should output verbose info from underlying models, etc.\n",
    "VERBOSE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11678989-3b16-489a-ac14-95d124f796bf",
   "metadata": {},
   "source": [
    "### Model params & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281f5de-6163-4b98-b409-e3b488fae58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file\n",
    "MODEL_FILE=\"./model/mistral-7b-openorca.Q5_K_M.gguf\"\n",
    "\n",
    "MODEL_CONTEXT_WINDOW=8192\n",
    "\n",
    "# Maximal lenght of model's output, in tokens.\n",
    "MAX_ANSWER_TOKENS = 2048\n",
    "\n",
    "# Chunk params in characters (not tokens).\n",
    "CHUNK_SIZE=10000\n",
    "CHUNK_OVERLAP=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45344f2-c4b3-44e6-b4dc-9d776242580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_FILE,\n",
    "    n_ctx=MODEL_CONTEXT_WINDOW,\n",
    "    # Number of tokens to process in parallel. Should be a number between 1 and n_ctx.\n",
    "    n_batch=512,\n",
    "    # Number of layers to be loaded into gpu memory. Default None.\n",
    "    n_gpu_layers=1,\n",
    "    # Maximal lenght of model's output, in tokens.\n",
    "    max_tokens=MAX_ANSWER_TOKENS,\n",
    "    # Don't be creative.\n",
    "    temperature=0,\n",
    "    verbose=VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e35d00-6c77-425d-9079-b3f31d8bad78",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc5bce-0bc4-4333-a369-937b999eb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_content():\n",
    "    \"\"\"Loads INPUT_FILE if set, otherwise returns INPUT_TEXT\"\"\"\n",
    "\n",
    "    if INPUT_FILE:\n",
    "        if INPUT_FILE.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(INPUT_FILE)\n",
    "            docs = loader.load()\n",
    "            print(f\"PDF: loaded {len(docs)} pages\")\n",
    "            return \"\\n\".join([d.page_content for d in docs])\n",
    "        \n",
    "        docs =  TextLoader(INPUT_FILE).load()\n",
    "        return docs[0].page_content\n",
    "\n",
    "    return INPUT_TEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8376f-1540-4d53-b06c-1984337f5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Write a summary of the following text delimited by tripple backquotes.\n",
    "{style}\n",
    "\n",
    "```{content}```\n",
    "\n",
    "{trigger} in {language}:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "Write a concise summary of the following:\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "def summarize_base(llm, content):\n",
    "    \"\"\"Summarize whole content at once. The content needs to fit into model's context window.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=VERBOSE)\n",
    "    output = chain.run(content)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def summarize_map_reduce(llm, content):\n",
    "    \"\"\"Summarize content potentially larger that model's context window using map-reduce approach.\"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.create_documents([content])\n",
    "    print(f\"Map-Reduce content splits ({len(split_docs)} splits): {[len(sd.page_content) for sd in split_docs]}\")\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(map_prompt_template)\n",
    "    combine_prompt = PromptTemplate.from_template(\n",
    "        combine_prompt_template\n",
    "    ).partial(\n",
    "        style=STYLE,\n",
    "        trigger=PROMPT_TRIGGER,\n",
    "        language=OUTPUT_LANGUAGE,\n",
    "    )\n",
    "\n",
    "    chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=map_prompt,\n",
    "        combine_prompt=combine_prompt,\n",
    "        combine_document_variable_name=\"content\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    output = chain.run(split_docs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1fb72-e0e8-4265-b1c1-0ad04bc74ba5",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d82f32-74a3-4d9c-aa27-4a4f3f200b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "content = load_content()\n",
    "content_tokens = llm.get_num_tokens(content)\n",
    "print(f\"Content length: {len(content)} chars, {content_tokens} tokens.\")\n",
    "print(\"Content sample:\\n\" + content[:200] + \"\\n\\n\")\n",
    "\n",
    "# Keep part of context window for models output.\n",
    "base_threshold = 0.75*MODEL_CONTEXT_WINDOW\n",
    "\n",
    "if (content_tokens < base_threshold):\n",
    "    print(\"Using summarizer: base\")\n",
    "    summary = summarize_base(llm, content)\n",
    "else:\n",
    "    print(\"Using summarizer: map-reduce\")\n",
    "    summary = summarize_map_reduce(llm, content)\n",
    "\n",
    "print(f\"Content length: {len(summary)} chars, {llm.get_num_tokens(summary)} tokens.\")\n",
    "print(\"Summary:\\n\" + summary + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360a902-0585-4a1f-8e1a-0cc06991487d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
